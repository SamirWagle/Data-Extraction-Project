{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Rows in file1 compared to file2:\n",
      "Empty DataFrame\n",
      "Columns: [file_name, label, value, x0, y0, x2, y2]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "\n",
    "# Load the two CSV files into DataFrames\n",
    "df_true = pd.read_csv(\"DE-TRAIN.csv\")\n",
    "df_pred = pd.read_csv(\"train.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Identify missing rows in file1 compared to file2\n",
    "missing_rows = df_true[~df_true.isin(df_pred)].dropna()\n",
    "\n",
    "# Display the missing rows\n",
    "print(\"Missing Rows in file1 compared to file2:\")\n",
    "print(missing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of x2  =88.71367570552314%\n",
      "Accuracy of y2  =99.28303780993932%\n",
      "Accuracy of x0  =95.69735212331014%\n",
      "Accuracy of y0  =99.43987607817769%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the common column names\n",
    "common_columns={'x0','y0','x2','y2'}\n",
    "\n",
    "# Initialize a dictionary to store Mean Absolute Errors for each column\n",
    "mae_dict = {}\n",
    "# Initialize an empty DataFrame with the desired columns\n",
    "accuracy_df = pd.DataFrame(columns=[f\"{common_column}_error\" for common_column in common_columns])\n",
    "\n",
    "for common_column in common_columns:\n",
    "    accuracy_df[f\"{common_column}error\"] = abs((df_true[f\"{common_column}\"] - df_pred[f\"{common_column}\"]) / df_true[f\"{common_column}\"])\n",
    "    # Print the resulting DataFrame with accuracy for each row\n",
    "\n",
    "for common_column in common_columns:\n",
    "    mean_value = accuracy_df[f\"{common_column}error\"].mean()\n",
    "    accuracy=1-mean_value\n",
    "    print(f\"Accuracy of {common_column}  ={accuracy*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of fieldvalue: 100.00%\n",
      "Accuracy of fieldlabel: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Specify the field for comparison (replace 'your_field' with the actual field name)\n",
    "field_to_compare = ['value','label']\n",
    "\n",
    "# Create a new column indicating whether the values are equal\n",
    "for field in field_to_compare:\n",
    "    df_pred['equal_values'] = (df_pred[f\"{field}\"] == df_true[f\"{field}\"]).astype(int)\n",
    "    # Calculate accuracy\n",
    "    accuracy = df_pred['equal_values'].mean() * 100\n",
    "    # Display accuracy\n",
    "    print(f\"Accuracy of field{field}: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3086927622.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[38], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    def accuracy(x1,x2)\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
